---
title: "Car Accident Severity Prediction"
author: "Takao"
date: "2022-12-05"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# TAKAO OBA

# Car Accident Prediction Project

(<https://www.kaggle.com/competitions/predicting-car-accidents-severity/overview>) According to kaggle.com, "This is a countrywide car accident dataset, which covers 49 states of the USA. The accident data are collected from February 2016 to Dec 2021, using multiple APIs that provide streaming traffic incident (or event) data. These APIs broadcast traffic data captured by a variety of entities, such as the US and state departments of transportation, law enforcement agencies, traffic cameras, and traffic sensors within the road-networks. Currently, there are about 2.8 million accident records in this dataset."

Further, this is the description for the data sets:

" The data is split into two data sets; the training data set which contains 35,000 observations with 44 variables. While the testing data contains 15,000 observations with 43 variables (No target variable).

-   The purpose of this project is to be able to classify accidents as "SEVERE" or "MILD" based on the rest of the data attributes.

-   The target variable is "SEVERITY" which is a categorical variable with two categories: "SEVERE" or "MILD". The "SEVERE" accidents are around 10% of the accidents, while "MILD" is the other 90% of the accidents in the training data.

-   This means, your classifier misclassification rate has to beat 10% to be considered a better than a chance classifier."

Ultimately, our goal is to undergo a systematic process to classify rather an accident is "SEVERE" or "MILD"

```{r}
library(tidyverse)
library(car)
library(dplyr)
library(gridExtra)
library(randomForest)
library(rpart)
library(tree)
library(caret)
library(class)
library(leaps)
```

```{r}
test <- read.csv("/Users/takaooba/Downloads/predicting-car-accidents-severity/AcctestNoYNew.csv")
test <- test[,-1]

train <- read.csv("/Users/takaooba/Downloads/predicting-car-accidents-severity/Acctrain.csv")

# head(train)
```

The dimensions can be found by the following

```{r}
dim(test)
dim(train)
```

Use below to examine the two data sets:

```{r}
# head(test)


# head(train)

# Column names of the two data sets
colnames(test)
colnames(train)

```

The numerical predictors are Start_Lat, Start_Lng, End_Lat, End_Lng, Distance.mi., Temperature.F., Wind_Chill.F., Humidity..., Pressure.in., Visibility.mi., Wind_Speed.mph. There are a total of 11 numerical predictors. This is both for the training and testing data.

The categorical predictors are Street, Side, City, Country, State, Zipcode, Country, Timezone, Airport_Code, Wind_Direction, Weather_Condition, Amenity, Bump, Crossing, Give_Way, Junction, No_Exit, Railway, Roundabout, Station, Stop, Traffic_Calming, Traffic Signal, Turning_Loop, Sunrise_Sunset, Civil_Twilight, Nautical_Twilight, Astronomical_Twilight There are a total of 29 categorical predictors. This is both for the training and testing data.

Next, I aim to impute missing values.

```{r}
# Testing Data
sum((is.na(test)))
```

```{r}
# Training Data
sum(is.na(train))
```

```{r}
# Total NA's in both data sets
sum((is.na(test))) + sum(is.na(train))
```

As a temporary step, omit the NA's value using na.omit and further assess the best predictors that will be used when constructing the model.

```{r}
train.1 <- na.omit(train)
# head(train.1)
train.1$SeverityNum <- ifelse(train.1$Severity == "MILD", 0, 1)
numericalpredictor <- train.1[,c(4,5,6,7,8,20,21,22,23,24,26,45)]
cor(numericalpredictor)
```

Based on the correlation plot that I have just created above, I have that the best predictors are Start_Lat, End_Lat, Start_Lng, End_Lng, Wine_Chill.F., Wind_Speed.mph.

Next, I plot graphs to see which predictors have distinct humps based on if the Severity is "MILD" or "SEVERE"

```{r}
library(ggplot2)
ggstart_lat <- ggplot(train.1, aes(Start_Lat, group = Severity, color = Severity  )) + geom_density()
ggstart_lng <- ggplot(train.1, aes(Start_Lng, group = Severity, color = Severity  )) + geom_density()
ggend_lat <- ggplot(train.1, aes(End_Lat, group = Severity, color = Severity  )) + geom_density()
ggend_lng <- ggplot(train.1, aes(End_Lng, group = Severity, color = Severity  )) + geom_density()
ggdistance <- ggplot(train.1, aes(Distance.mi., group = Severity, color = Severity  )) + geom_density()
ggtemperature <- ggplot(train.1, aes(Temperature.F., group = Severity, color = Severity  )) + geom_density()
gghumidity <- ggplot(train.1, aes(Humidity..., group = Severity, color = Severity  )) + geom_density()
ggpressure <- ggplot(train.1, aes(Pressure.in., group = Severity, color = Severity  )) + geom_density()
ggvisibility <- ggplot(train.1, aes(Visibility.mi., group = Severity, color = Severity  )) + geom_density()
ggwind_speed <- ggplot(train.1, aes(Wind_Speed.mph., group = Severity, color = Severity  )) + geom_density()
ggwind_chill <- ggplot(train.1, aes(Wind_Chill.F., group = Severity, color = Severity  )) + geom_density()

library(gridExtra)
grid.arrange(ggstart_lat, ggstart_lng, ggend_lat)
grid.arrange(ggend_lng, ggdistance, ggtemperature)
grid.arrange(gghumidity, ggpressure, ggvisibility, ggwind_speed, ggwind_chill)
```

Based on the above graphs, I have that the 6 best numerical variables are Start_Lat, End_Lat, Start_Lng, End_Lng, Wind_Chill.F., Temperature.F.

I will continue to plot these 6 numerical variables

```{r}
grid.arrange(ggstart_lat, ggend_lat)
grid.arrange(ggstart_lng, ggend_lng)
grid.arrange(ggwind_chill, ggtemperature)
```

Next, I will construct stacked bar charts for the best three categorical predictors based on the response variables.

```{r}

# 
# Street, Side, City, Country, State, Zipcode, Country, Timezone, Airport_Code, Wind_Direction, Weather_Condition, Amenity, Bump, Crossing, Give_Way, Junction, No_Exit, Railway, Roundabout, Station, Stop, Traffic_Calming, Traffic Signal, Turning_Loop, Sunrise_Sunset, Civil_Twilight, Nautical_Twilight, Astronomical_Twilight

# However, I will need to determine which predictors makes sense in the first place in the context of traffic accidents

ggside <- ggplot(train.1, aes(Side, group = Severity, color = Severity  , fill = Severity)) + geom_bar()
ggstate <- ggplot(train.1, aes(State, group = Severity, color = Severity  , fill = Severity)) + geom_bar()
ggtimezone <- ggplot(train.1, aes(Timezone, group = Severity, color = Severity  , fill = Severity)) + geom_bar()
ggwinddirection <- ggplot(train.1, aes(Wind_Direction, group = Severity, color = Severity  , fill = Severity)) + geom_bar()
ggamenity <- ggplot(train.1, aes(Amenity, group = Severity, color = Severity  , fill = Severity)) + geom_bar()
ggbump <- ggplot(train.1, aes(Bump, group = Severity, color = Severity , fill = Severity )) + geom_bar()
ggcrossing <- ggplot(train.1, aes(Crossing, group = Severity, color = Severity , fill = Severity )) + geom_bar()
gggiveway <- ggplot(train.1, aes(Give_Way, group = Severity, color = Severity , fill = Severity )) + geom_bar()
ggjunction <- ggplot(train.1, aes(Junction, group = Severity, color = Severity , fill = Severity )) + geom_bar()
ggnoexit <- ggplot(train.1, aes(No_Exit, group = Severity, color = Severity , fill = Severity )) + geom_bar()
ggrailway <- ggplot(train.1, aes(Railway, group = Severity, color = Severity  , fill = Severity)) + geom_bar()
ggroundabout <- ggplot(train.1, aes(Roundabout, group = Severity, color = Severity  , fill = Severity)) + geom_bar()
ggstation <- ggplot(train.1, aes(Station, group = Severity, color = Severity , fill = Severity )) + geom_bar()
ggstop <- ggplot(train.1, aes(Stop, group = Severity, color = Severity , fill = Severity )) + geom_bar()
ggcalm <- ggplot(train.1, aes(Traffic_Calming, group = Severity, color = Severity  , fill = Severity)) + geom_bar()
ggsignal <- ggplot(train.1, aes(Traffic_Signal, group = Severity, color = Severity , fill = Severity )) + geom_bar()
ggloop <- ggplot(train.1, aes(Turning_Loop, group = Severity, color = Severity , fill = Severity )) + geom_bar()   # There is only one boolean value so I will omit this predictor 
ggsunset<- ggplot(train.1, aes(Sunrise_Sunset, group = Severity, color = Severity  , fill = Severity)) + geom_bar()
ggcivil <- ggplot(train.1, aes(Civil_Twilight, group = Severity, color = Severity , fill = Severity )) + geom_bar()
ggnautical <- ggplot(train.1, aes(Nautical_Twilight, group = Severity, color = Severity , fill = Severity )) + geom_bar()
ggastronomical <- ggplot(train.1, aes(Astronomical_Twilight, group = Severity, color = Severity , fill = Severity )) + geom_bar()


library(gridExtra)
grid.arrange(ggside, ggstate, ggtimezone,ggwinddirection)
grid.arrange(ggamenity, ggbump,ggcrossing, gggiveway)
grid.arrange(ggjunction, ggnoexit, ggrailway, ggroundabout) 
grid.arrange(ggstation, ggstop, ggcalm, ggsignal)
grid.arrange(ggloop, ggsunset, ggcivil, ggnautical)
grid.arrange(ggastronomical)
```

I aim to look at the proportion of the Severity (either MILD or SEVERE) between the different bars. I want to see the proportion of the MILD and SEVERE to be different between the bars and will look at these proportions to determine the best predictors.

The best categorical predictors based on above are Timezone, Give_Way, and Traffic_Signal.

## Creating a testing data set within the training set

```{r}
set.seed(717)
sam <- sample(1:35000, 5000, replace = F)
train.x <- train[,-1]
train.y <- train[,1]

# Fitting values into training and testing set (x and y)
train_sam_x <- train.x[-sam,]
test_sam_x <- train.x[sam,]
train_sam_y <- train.y[-sam]
test_sam_y <- train.y[sam]


head(train_sam_x)
head(train_sam_y)

```

## Inputing missing values

To visualize which predictors has the most missing values, I will be looking at below.

```{r}
library(VIM)
acc_plot <- aggr(train, col=c('navyblue','yellow'), numbers=TRUE, sortVars=TRUE, labels=names(train), cex.axis=.7, gap=3, ylab=c("Missing data","Pattern"))

```

To better impute the variables, I will temporarily combine the testing and training data.

```{r}

dim(test)
dim(train.x)

comb.df <- rbind(train.x, test)
head(comb.df)
dim(comb.df)

```

```{r}

numcomb.df <- comb.df[,c(3,4,5,6,7,19,20,21,22,23,25)]
colSums(is.na(numcomb.df))
median1 <- apply(numcomb.df, 2, median, na.rm = TRUE)

dim(numcomb.df)

# Imputing the corresponding median values, I have that

# Imputing Mean
# for (i in 1:dim(numericaltest)[2]){
#   for (j in 1:dim(numericaltest)[1]){
#     if(is.na(numericaltest[j,i]) == TRUE){
#       numericaltest[j,i] <- as.numeric(mean(na.omit(numericaltest[,i])))
#     }
#   }
# }


# # Imputing median
for (i in 1:dim(numcomb.df)[2]){
  for (j in 1:dim(numcomb.df)[1]){
    if(is.na(numcomb.df[j,i]) == TRUE){
      numcomb.df[j,i] <- as.numeric(median1[i])
    }
  }
}


# I can see that the amount of NA's in this data frame is now zero
colSums(is.na(numcomb.df))

```

```{r}
# Achieve the numerical predictors from the training and testing data
head(numcomb.df)
numericaltrain <- numcomb.df[1:35000,]
numericaltest <- numcomb.df[35001:50000,]
train.x <- train[,1]

library(class)
# take the square root of the number of predictors to find the optimal value of k.
# Perform a knn test with k = 3 and k = 4
knn.model <- knn(numericaltrain, numericaltest,cl = train.x, k = 3)
table(knn.model)
# write.csv(knn.model,"knn.model.3.csv")
knn.model <- knn(numericaltrain, numericaltest,cl = train.x, k = 4)
table(knn.model)
# write.csv(knn.model,"knn.model.3.csv")
```

I have found that the knn model does not product the best results, so I will look into alternative methods.

## Decision Tree

```{r}
# I will construct the data frame that will be utilized for the decision tree
Severity <- train.x
traintemp <- cbind(Severity, numericaltrain)

# Construction of the model
train_tree <- tree(factor(Severity) ~ ., data = traintemp)
summary(train_tree)

# Plot and label the tree
plot(train_tree)
text(train_tree, pretty = 0)


# Resplit the training data and assess the misclassification rate for training data
Severity <- Severity[sam]
traintemp <- cbind(Severity, numericaltrain[sam,])

train_tree <- tree(factor(Severity) ~ ., data = traintemp)
temp <-  predict(train_tree, newdata = test_sam_x, type = "class")
length(test_sam_y)
length(temp)
table(temp, test_sam_y)
mean(temp != test_sam_y)
```

## Pruning the tree

```{r}
# As I see from the decision tree above, everything will be classified as MILD. This is not a very good prediction model, so I will utilize a pruned decision tree
train_cv = cv.tree(train_tree, FUN = prune.misclass)
names(train_cv)
summary(train_cv)

plot(train_cv$size, train_cv$dev)

train_pruned <- prune.misclass(train_tree, best = 5)
plot(train_pruned)
text(train_pruned, pretty = T)
```

I have pruned the decision tree, but it is still the exactly the same as the initial decision tree. I will look into alternative models.

Split the training data into testing and training to assess accuracy of model prior to submissions

```{r}
set.seed(1128)
sam <- sample(1:35000, 5000, replace = F)
train_sam <- train[-sam,]
test_sam <- train[sam,]
traintemp_sam <- traintemp[-sam,]
testtemp_sam <- traintemp[sam,]
testtemp_sam_x <- testtemp_sam[,-1]

traintest_tree <- tree(formula = factor(Severity)~., data = traintemp_sam)
test_prediction <- predict(traintest_tree, newdata = testtemp_sam, type = "class")

# Building confusion matrix
table((testtemp_sam$Severity), (test_prediction))

# misclassification rate based on the training set is 
mean((testtemp_sam$Severity)!=(test_prediction))
```

Uncomment the code chunk below to generate csv file that will be submitted to Kaggle

```{r}
# test_prediction <- predict(train_pruned, data = train_pred, newdata = test_pred, type = "class")
# table(test_prediction)
# write.csv(test_prediction, "tree_model_1.csv")
```

## Adding predictors

### timediff

I have seen that thus far, I have seen high misclassification rates. I will look into discovering new predictors. Below, I will be looking into generating a time difference predictor. Intuitively, if the duration of the accident is longer, it is more likely that the accident is SEVERE since there could possibly be a greater collision, greater amount of assistance needed, and greater cleaning needed.

```{r}
# To make the time difference predictor

head(train$Start_Time, 5)

# Notice that all time stamps are trapped with character T and Z.
start <- strsplit(train$Start_Time, "T|Z")
end <- strsplit(train$End_Time, "T|Z")

for(i in 1:length(end)) {
  end[i] <- paste(end[[i]][1], end[[i]][2], sep = " ")
}
for(i in 1:length(start)) {
  start[i] <- paste(start[[i]][1], start[[i]][2], sep = " ")
}


# Calculate the difference in time through a R function
time_diff <- difftime(unlist(end), unlist(start), unit = "hours")

# Adding to the training model as a new predictor
train$time_diff <- time_diff

# repeat for test data

start <- strsplit(test$Start_Time, "T|Z")
end <- strsplit(test$End_Time, "T|Z")

for(i in 1:length(end)) {
  end[i] <- paste(end[[i]][1], end[[i]][2], sep = " ")
}
for(i in 1:length(start)) {
  start[i] <- paste(start[[i]][1], start[[i]][2], sep = " ")
}

time_diff <- difftime(unlist(end), unlist(start), unit = "hours")

# Adding to the testing model as a new predictor
test$time_diff <- time_diff
```

### rush_hour

Further, I look into the specific hours that I expect there to be a rush hour. This is mainly when workers leave their work place to go back home. Ultimately, there are more individuals on the road during these hours

```{r}
# trying to determine which observations are in the rush hours

rush <- c()

# Iterating over each observation, examining the time that the accident occurred
for(i in 1:dim(train)[1]) {
  s <- strsplit(train$Start_Time[i], "T|Z")[[1]]
  seven_pm <- paste(s[1], "17:00:00", sep = " ")
  # four_pm <- paste(s[1], "14:00:00", sep = " ")
  start <- substring(gsub("T|Z", " ", train$Start_Time[i]), 1, nchar(seven_pm))
  before_7 <- difftime(seven_pm, start, unit = "hours")
  # after_4 <- difftime(start, four_pm, unit = "hours")
  if(0 <= before_7 && 3 >= before_7) {
    rush[i] <- TRUE
  } else {
    rush[i] <- FALSE
  }
}

head(rush, 15)
# head(train)

# Add the generated vector as a predictor
train$rush_hour <- rush


# repeat for testing data

rush <- c()
for(i in 1:dim(test)[1]) {
  s <- strsplit(test$Start_Time[i], "T|Z")[[1]]
  seven_pm <- paste(s[1], "17:00:00", sep = " ")
  # four_pm <- paste(s[1], "14:00:00", sep = " ")
  start <- substring(gsub("T|Z", " ", test$Start_Time[i]), 1, nchar(seven_pm))
  before_7 <- difftime(seven_pm, start, unit = "hours")
  # after_4 <- difftime(start, four_pm, unit = "hours")
  if(0 <= before_7 && 3 >= before_7) {
    rush[i] <- TRUE
  } else {
    rush[i] <- FALSE
  }
}

test$rush_hour <- rush
# head(test)
```

### precovid, covid, postcovid

The dataset consists of observations from various years. Mainly in 2019 and 2020, the COVID-19 pandemic has drastically changed the society. More individuals stayed at home to finish their tasks which indicates that less individuals were on the road. I aim to understand when the observation had occurred pre-covid, during covid, or post-covid.

```{r}
# manipulate training data
train$Date <- as.Date(train$Start_Time)
train$precovid <- ifelse(train$Date < as.Date("2020-04-01"), 1, 0)
train$covid <- ifelse(train$Date > as.Date("2020-04-01") & train$Date < as.Date("2021-01-01"), 1, 0)
train$postcovid <- ifelse(train$Date > as.Date("2021-01-01"), 1, 0)

# do same thing to testing

test$Date <- as.Date(test$Start_Time)
test$precovid <- ifelse(test$Date < as.Date("2020-04-01"), 1, 0)
test$covid <- ifelse(test$Date > as.Date("2020-04-01") & test$Date < as.Date("2021-01-01"), 1, 0)
test$postcovid <- ifelse(test$Date > as.Date("2021-01-01"), 1, 0)
```

### Description RegEx

In the dataset, there is a column dedicated for the description of the accident. Closely examining the explanations, accidents that are classified as SEVERE tend to have certain vocabularies. Ultimately, I want to assess which observations has these key words.

```{r}
# Key words in SEVERE accidents are Road, alternate, caution, closed, blocked, incident

# Make a column for each of these words
train <- train %>%
  mutate(road = as.numeric(str_detect(Description, "(?i)Road")),
         altrt = as.numeric(str_detect(Description, "(?i)alternate")),
         caution = as.numeric(str_detect(Description, "(?i)caution")),
         closed = as.numeric(str_detect(Description, "(?i)closed")),
         blocked = as.numeric(str_detect(Description, "(?i)blocked")),
         incident = as.numeric(str_detect(Description, "(?i)incident")))
# head(train)

# Perform the same for the testing data
test <- test %>%
  mutate(road = as.numeric(str_detect(Description, "(?i)Road")),
         altrt = as.numeric(str_detect(Description, "(?i)alternate")),
         caution = as.numeric(str_detect(Description, "(?i)caution")),
         closed = as.numeric(str_detect(Description, "(?i)closed")),
         blocked = as.numeric(str_detect(Description, "(?i)blocked")),
         incident = as.numeric(str_detect(Description, "(?i)incident")))
# head(test)
```

Performing randomForest using RegEx, Covid date, and rush hour that was generated thus far

```{r}
# take all the significant predictors that we generated, exclude time and insignificant predictors
train_mod <- train[, c(1, 45, 46, 48, 49, 50, 51, 52, 53, 54, 55, 56)]
set.seed(1128)
sam <- sample(1:35000, 5000, replace = F)
test_sample <- train_mod[sam,]
train_sample <- train_mod[-sam,]

# take all the significant predictors that we generated, exclude time and insignificant predictors
test_mod <- test[, c(44,45, 47, 48, 49, 50, 51, 52, 53, 54, 55)]


# Performing random forest
training_tree <- randomForest(as.factor(Severity) ~ ., data = train_mod, mtry = 6, importance = T)

training_tree_pred_tr <- predict(training_tree, data = train_sample, newdata = test_sample)
# training_tree_pred_tr

# confusion matrix
table(test_sample$Severity, training_tree_pred_tr)
# misclassification rate
mean(test_sample$Severity != training_tree_pred_tr)
```

Uncomment below when generating the Severity for the actual testing data

```{r}
# testing_tree_pred <- predict(training_tree, data = train_mod, newdata = test_mod)
# write.csv(testing_tree_pred, "randForest_rush.csv")
```

### Zipcode

Accidents may tend to be more SEVERE in certain regrions and towns. To assess the specific cities, I will look into the Zipcode.

```{r}
# Which zipcode had the most amount of SEVERE accidents
head(sort(table(train[train$Severity == "SEVERE",]$Zipcode), decreasing = T),15)
# Below are the most common zipcodes that had severe accidents

top_zip <- c(75243, 80229, 33168, 75228, 80216, 80401, 37210, 60607, 20020, 33127, 33150, 60621, 77092, 80104, 80118)
train <- train %>%
   mutate(
    newzip = str_detect(Zipcode, "75243|80229|33168|75228|80216|80401|37210|60607|20020|33127|33150|60621|77092|80104|80118"),
  )

test <- test %>%
   mutate(
    newzip = str_detect(Zipcode, "75243|80229|33168|75228|80216|80401|37210|60607|20020|33127|33150|60621|77092|80104|80118"),
  )

# generating a new predictor
train_mod$newzip <- train$newzip
test_mod$newzip <- test$newzip
```

Further, from external sources, the zipcodes with the most frequent SEVERE accidents is extracted.

```{r}
# According to [carinsurance.com](https://www.carinsurance.com/Articles/car-insurance-rate-comparison.aspx#:~:text=Most%20and%20least%20expensive%20ZIP,NY%20and%20New%20Orleans%2C%20LA) the following zipcodes will be looked into

# 48210|11212|70145|63107|33603|91205|19801|19142|89106|2119

train <- train %>%
  mutate(
    online_zip = str_detect(Zipcode, "48210|11212|70145|63107|33603|91205|19801|19142|89106|2119"),
  )
test <- test %>%
  mutate(
    online_zip = str_detect(Zipcode, "48210|11212|70145|63107|33603|91205|19801|19142|89106|2119"),
  )

# generating new predictors
train_mod$online_zip <- train$online_zip
test_mod$online_zip <- test$online_zip
```

### Holidays

Many individuals go outside to have the time of their life during notable holidays. As a result, many do not look into consequences and drink-and-drive. Assess the major holidays and extract accidents that occur during these days.

Add in Holidays (Memorial day, mothers and fathers day, and fourth of July) Our dates are from 2016 to 2021, so we need to find the days of memorial day for each year as the date changes

2016: 2016-05-30 2017: 2017-05-29 2018: 2018-05-28 2019: 2019-05-27 2020: 2020-05-25 2021: 2021-05-31

Fourth of July Just July Fourth of each year

Mothers day 2016: 2016-05-08 2017: 2017-05-14 2018: 2018-05-13 2019: 2019-05-12 2020: 2020-05-10 2021: 2021-05-09

Fathers day 2016: 2016-06-19 2017: 2017-06-18 2018: 2018-06-17 2019: 2019-06-16 2020: 2020-06-21 2021: 2021-06-20

```{r}
train <- train %>%
  mutate(holiday = str_detect(Start_Time,
                              "2016-05-30|2017-05-29|2018-05-28|2019-05-27|2020-05-25|2021-05-31|2016-07-04|2017-07-04|2018-07-04|2019-07-04|2020-07-04|2021-07-04|2016-05-08|2017-05-14|2018-05-13|2019-05-12|2020-05-10|2021-05-09|2016-06-19|2017-06-18|2018-06-17|2019-06-16|2020-06-21|2021-06-20"),
         )
test <- test %>%
  mutate(holiday = str_detect(Start_Time,
                              "2016-05-30|2017-05-29|2018-05-28|2019-05-27|2020-05-25|2021-05-31|2016-07-04|2017-07-04|2018-07-04|2019-07-04|2020-07-04|2021-07-04|2016-05-08|2017-05-14|2018-05-13|2019-05-12|2020-05-10|2021-05-09|2016-06-19|2017-06-18|2018-06-17|2019-06-16|2020-06-21|2021-06-20"),
         )

# generating new predictor
train_mod$holiday <- train$holiday
test_mod$holiday <- test$holiday
```

### crash_months

Upon examination, certain months tend to have a higher SEVERE accident rate. This is mainly during the summer and perhaps this nature occurs because of school breaks and as a result, the amount of individuals who travel increases.

```{r}
train$crash_months <- ifelse(str_detect(substring(as.Date(train$Start_Time), 6, 7), "06|07|08|09|10"), 1, 0)
test$crash_months <- ifelse(str_detect(substring(as.Date(test$Start_Time), 6, 7), "06|07|08|09|10"), 1, 0)

train$online_zip <- ifelse(is.na(train$online_zip), F, train$online_zip)
train$newzip <- ifelse(is.na(train$newzip), F, train$newzip)


# these observations have missing values, imputation of 0 suffices because upon examination, they are not significant
which(is.na(train_mod$newzip))
train_mod[c(1523,3855,3855,5085,10469,11018,11856,12796,14377,15091,15939,16612,17226,20130,20330,21007,22279,25447,32080),]$newzip <- 0
train_mod[c(1523,3855,3855,5085,10469,11018,11856,12796,14377,15091,15939,16612,17226,20130,20330,21007,22279,25447,32080),]$online_zip <- 0

```

Upon generating these predictors, as a temporary step, I generated a model again and the accuracy did go up on Kaggle!

```{r}
tree_model <- randomForest(as.factor(Severity) ~ ., data = train_mod, mtry = 7, importance = T)
0.0674 # 6
0.0668 # 7

tree_model_predict <- predict(tree_model, newdata = test_mod)
table(tree_model_predict)

# write.csv(tree_model_predict, "randomForest_mtry7.csv")
```

### insurance, teendrivers, car crashes fatalities teens

The accuracy thus far is a 0.93235. To increase our testing accuracy, again I look into external sources to see various factors. Three predictors are introducted below. First is insurance which looks into states that have the highest insurance prices. Second is the teen drivers to see which state has the highest proportion of teen drivers. Third is the car crashes fatalities teen which indicates the state with the highest fatalities (SEVERE) amongst teens.

```{r}
# The following sources were used
# Insurance: [experian.com](https://www.experian.com/blogs/ask-experian/research/most-expensive-states-for-car-insurance/)
# Teen Drivers: [carinsurance.com](https://www.carinsurance.com/Articles/teen-driving-safety-least-and-most-dangerous-states.aspx)
# Car Crashes Fatalities Teens: [iihs.org](https://www.iihs.org/topics/fatality-statistics/detail/teenagers)

train <- train %>%
   mutate(
    insurance = str_detect(State, "DE|LO|NY|GA|MD|MI|NJ|FL|RI|SC"),
    teendrivers = str_detect(State, "KY|MS|NC|MO|WV"),
    ccft = str_detect(State, "WY|SD|MS|MO|AL")
  )

# generating the predictors for training
train_mod$insurance <- train$insurance
train_mod$teendrivers <- train$teendrivers
train_mod$ccft <- train$ccft

test <- test %>%
   mutate(
    insurance = str_detect(State, "DE|LO|NY|GA|MD|MI|NJ|FL|RI|SC"),
    teendrivers = str_detect(State, "KY|MS|NC|MO|WV"),
    ccft = str_detect(State, "WY|SD|MS|MO|AL")
  )

# generating the predictors for testing
test_mod$insurance <- test$insurance
test_mod$teendrivers <- test$teendrivers
test_mod$ccft <- test$ccft
```

### State Death Rate

Accident mortality rate differs by state. Upon utilizing external sources, a predictor is generated to assess if an accident is inclined to be more SEVERE if it occured at a given state.

```{r}
# How many fatal car crashes per 100,000 people in the state's population?


# Source for State Death Rate: [cdc.gov](https://www.cdc.gov/nchs/pressroom/sosmap/accident_mortality/accident.htm)

state_death_rate <- c()
for(i in 1:35000) {
  # Let's split the states into three categories of deaths per 100,000 population: high, mid, low 
  if(str_detect(train$State[i], "CT|DC|MA|MN|NH|NJ|NY|PA|UT|WA")) {
    state_death_rate[i] <- "Low"
  } else if(str_detect(train$State[i], "AZ|CA|CO|DE|ID|IL|IN|IA|KA|MA|MD|MI|NE|NV|NC|ND|OH|OR|TX|VT|VA|WV|WI")) {
    state_death_rate[i] <- "Mid"
  } else {
    state_death_rate[i] <- "High"
  }
}

train$state_death_rate <- state_death_rate
train_mod$sdr <- train$state_death_rate
# repeat for testing data

state_death_rate <- c()
for(i in 1:15000) {
  if(str_detect(test$State[i], "CT|DC|MA|MN|NH|NJ|NY|PA|UT|WA")) {
    state_death_rate[i] <- "Low"
  } else if(str_detect(test$State[i], "AZ|CA|CO|DE|ID|IL|IN|IA|KA|MA|MD|MI|NE|NV|NC|ND|OH|OR|TX|VT|VA|WV|WI")) {
    state_death_rate[i] <- "Mid"
  } else {
    state_death_rate[i] <- "High"
  }
}


test$state_death_rate <- state_death_rate
test_mod$sdr <- test$state_death_rate


```

### Temperature

At the beginning, I have assessed the significant quantitative predictors. Temperature during the given accident is one of the most significant predictor.

```{r}
# These are the column indeces for temperature and wind chill
temp <- train[,c(20,21)]
temp1 <- test[,c(19,20)]

tempdf <- rbind(temp,temp1)



# temporary data cleaning 
median1 <- apply(tempdf, 2, median, na.rm = TRUE)
for (i in 1:dim(tempdf)[2]){
  for (j in 1:dim(tempdf)[1]){
    if(is.na(tempdf[j,i]) == TRUE){
      tempdf[j,i] <- as.numeric(median1[i])
    }
  }
}


colSums(is.na(tempdf))

temptrain <- tempdf[1:35000,]
temptest <- tempdf[35001:50000,]

```

However, there are many missing or NA values for this predictor.

Below, I perform the missForest imputation to tackle this problem.

```{r}
# install.packages("missForest")
library(missForest)
```

```{r}
# changing the qualitative predictors to factor for the missForest imputation
temptemp <- train_mod
# colSums(is.na(temptemp))
temptemp$rush_hour <- as.factor(temptemp$rush_hour)
temptemp$precovid <- as.factor(temptemp$precovid)
temptemp$covid <- as.factor(temptemp$covid)
temptemp$postcovid <- as.factor(temptemp$postcovid)
temptemp$road <- as.factor(temptemp$road)
temptemp$altrt <- as.factor(temptemp$altrt)
temptemp$caution <- as.factor(temptemp$caution)
temptemp$closed <- as.factor(temptemp$closed)
temptemp$blocked <- as.factor(temptemp$blocked)
temptemp$incident <- as.factor(temptemp$incident)
temptemp$newzip <- as.factor(temptemp$newzip)
temptemp$online_zip <- as.factor(temptemp$online_zip)
temptemp$holiday <- as.factor(temptemp$holiday)
temptemp$insurance <- as.factor(temptemp$insurance)
temptemp$teendrivers <- as.factor(temptemp$teendrivers)
temptemp$ccft <-  as.factor(temptemp$ccft)
temptemp$sdr <- as.factor(temptemp$sdr)
temptemp$Temperature.F. <- train$Temperature.F.
temptemp1 <- temptemp[,-c(1,2)]
```

Performing the imputation upon changing the class of each predictors

```{r}
temptemp2 <- missForest(temptemp1)
```

Successful imputation is shown below

```{r}
temp3 <- temptemp2$ximp
head(temp3)

# generating temperature predictor
train_mod$Temperature.F. <- temp3$Temperature.F.

```

To assess work thus far, the model is implemented

Testing the model on training data which is re-split into training and testing.

```{r}
set.seed(1128)
sam <- sample(1:35000, 5000, replace = F)
train_sam <- train_mod[-sam,]
test_sam <- train_mod[sam,]

training_tree <- randomForest(as.factor(Severity) ~ ., data = train_sam, mtry = 6, importance = T)
testing_tree_pred <- predict(training_tree, data = train_sam, newdata = test_sam)

# Confusion Matrix
table(testing_tree_pred, test_sam$Severity)

# Misclassification rate
mean(testing_tree_pred != test_sam$Severity)
```

Uncomment below when generating the Severity for the actual testing data

```{r}
# training_tree <- randomForest(as.factor(Severity) ~ ., data = train_mod, mtry = 6, importance = T)
# summary(training_tree)
# test_mod$Temperature.F. <- temptest$Temperature.F.
# testing_tree_pred <- predict(training_tree, data = train_mod, newdata = test_mod)
```

Writing to our csv file that will be submitted to Kaggle

```{r}
# write.csv(testing_tree_pred, "randForest_rush8.csv")
```

### diff_lat, diff_lng

Similar to the temperature, the latitude and longitude were extremely significant predictors. They will be incorporated as predictors in the form of difference as if both of them are used, multicollinearity can be introduced.

```{r}
# Start_Lat, End_Lat, Start_Lng, End_Lng
train_mod$diff_lat <- train$Start_Lat - train$End_Lat
train_mod$diff_lng <- train$Start_Lng - train$End_Lng


test_mod$diff_lat <- test$Start_Lat - test$End_Lat
test_mod$diff_lng <- test$Start_Lng - test$End_Lng
```

## Final Model Implementation using Logistic Regression

```{r}
train_mod_sam <- train_mod[-sam,]
test_mod_sam <- train_mod[sam,]

# testing model using samples

lda_train_sam <- glm(as.factor(Severity) ~ ., family = binomial(), data = train_mod_sam)

lda_test_predict_sam <- predict(lda_train_sam, newdata = test_mod_sam, type = "response")
pred_test_sam <- rep("MILD", length(lda_test_predict_sam))
pred_test_sam[lda_test_predict_sam >= 0.5] <- "SEVERE"


table(pred_test_sam, test_mod_sam$Severity)
mean(pred_test_sam != test_mod_sam$Severity)
```

Uncomment below when generating the Severity for the actual testing data

```{r}
# actual testing prediction for submission
# lda_train <- glm(as.factor(Severity) ~ ., family = binomial(), data = train_mod)
# test_mod$Temperature.F. <- numericaltest$Temperature.F.
# 
# lda_test <- predict(lda_train, newdata = test_mod, type = "response")
# pred_test <- rep("MILD", length(lda_test))
# pred_test[lda_test >= 0.5] <- "SEVERE"
# 
# write.csv(pred_test, "logistic_regression_kaggle.csv")
```

## Final Model Implementation using randomForest

Testing the model on training data which is resplit into training and testing.

```{r}
training_tree <- randomForest(as.factor(Severity) ~ ., data = train_mod_sam, mtry = 6, importance = T)
testing_tree_pred <- predict(training_tree, data = train_mod_sam, newdata = test_mod_sam)

table(testing_tree_pred, test_mod_sam$Severity)
mean(testing_tree_pred != test_mod_sam$Severity)
```

To look at which predictors are the most significant, we have that
```{r}
varImpPlot(training_tree)
```


Uncomment below when generating the Severity for the actual testing data

```{r}
# training_tree <- randomForest(as.factor(Severity) ~ ., data = train_mod, mtry = 6, importance = T)
# summary(training_tree)
# testing_tree_pred <- predict(training_tree, data = train_mod, newdata = test_mod)
# write.csv(train_mod, "train_mod.csv")
# write.csv(test_mod, "test_mod.csv")
```

As a result, the constructed model has generated a 0.93688 accuracy! Upon systematic process, the model accuracy has gradually increased.

## Below are hidden code chunks that was used during experimentations (failed models and execution, testing)

Attempting to use weather as a predictor - did not generate a good model

```{r}
# test_num <- data.frame(scale(numcomb.df))
# test_pred <- test_num
# 
# acc.test$Weather_Condition <- ifelse(is.na(acc.test$Weather_Condition), "Fair", acc.test$Weather_Condition)
# 
# 
# weather <- function(x) {
#   weather_list <- list()
#   for(i in 1:dim(x)[1]) {
#     if(str_detect(x$Weather_Condition[i], "Clear|Fair|Mostly Cloudy|Drizzle|Light Drizzle|Partly Cloudy|Scattered Clouds|Patches of Fog|Shallow Fog")) {
#       weather_list[i] <- "Light"
#     } else if(str_detect(x$Weather_Condition[i], "Blowing Dust / Windy|Blowing Dust| Cloudy / Windy|Drizzle / Windy|Fair / Windy|Fog|Fog / Windy|Haze|Haze / Windy|Light Rain Shower / Windy|Mostly Cloudy / Windy|Partly Cloudy / Windy|Squalls / Windy|Drizzle and Fog|Rain Shower|Overcast")) {
#       weather_list[i] <- "Mid"
#     } else if(str_detect(x$Weather_Condition[i], "Freezing Rain|Heavy Drizzle|Heavy Rain|Heavy Rain / Windy|Heavy T-Storm|Heavy T-Storm / Windy|Heavy Thunderstorms and Rain|Light Rain Shower|Light Rain with Thunder|Rain|Rain / Windy|Rain Shower|Showers in the Vicinity|Thunderstorms and Rain")) {
#       weather_list[i] <- "Rain"
#     } else {
#       weather_list[i] <- "Heavy"
#     }
#   }
#   return(as.character(weather_list))
# }
# 
# 
# 
# test_pred$weather <- as.factor(weather(comb.df))

```

Failed kNN imputation

```{r}
# Attempting to do kNN impute


# colSums(is.na(test))
# tempcol1 <- as.data.frame(temp[,1])
# tempcol2 <- as.data.frame(temp[,2])
#
# tempcol1
#
# library(caret)
#
# preProcValues <- preProcess(tempcol2,
#                             method = "knnImpute",
#                             k = 20,
#                             knnSummary = mean)
# preProcValues
#
#
# idx <- apply(tempcol2, 1, function(x) sum(is.na(x)))
# length(as.vector(which(idx == ncol(tempcol2))))
#
#
# tempinfo <- predict(preProcValues, tempcol1)
# tempinfo
```

Attempted to use Astronomical_Twilight as a predictor - did not generate best model

```{r}
# table(train_mod$Astronomical_Twilight)
# 
# train_mod$Astronomical_Twilight <- ifelse(is.na(train_mod$Astronomical_Twilight), "Day", "Night")
# test_mod$Astronomical_Twilight <- ifelse(is.na(test_mod$Astronomical_Twilight), "Day", "Night")
# 
# test_mod$weather <- weather(test)
# 
# 
# 
# train_mod <- train[, c(1, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53)]
# head(train_mod)
# 
# 
# test_mod <- test[, c(44, 45, 46, 47, 48, 49, 50, 51, 52, 53)]
# 
# tree_train <- tree(as.factor(Severity) ~., data = train_mod)
# 
# plot(tree_train)
# text(tree_train, pretty = 0)
# 
# tree_train_1 <- predict(tree_train, data = train_mod, newdata = test_mod, type = "class")
# 
# table(tree_train_1, test_sam$Severity)
# mean(tree_train_1 != test_sam$Severity)
# 
# length(tree_train_1)
# 
# 
# test_sam <- train_mod[sam, ]
# train_sam <- train_mod[-sam, ]
# 
# write.csv(tree_train_1, "tree_model_time_difference.csv")

```

Attempted to detect "Road closed" - predictor already exist, introduces multicollinearity

```{r}
# t <- train_sam[, c(1, 47, 48, 49, 50, 51, 52)]
# t_test <- test_sam[, c(1, 47, 48, 49, 50, 51, 52)]
# 
# tree_t <- tree(as.factor(Severity) ~ ., data = t)
# 
# head(test)
# 
# plot(tree_t)
# text(tree_t, pretty = 0)
# 
# t_predict <- predict(tree_t, data = t, newdata = test, type = "class")
# table(t_predict, t_test$Severity)
# mean(t_predict != t_test$Severity)
# 
# train[train$Severity == "SEVERE",c(1, 9)]
# train[train$Severity == "MILD",c(1, 9)]
# str_detect(train[16,9], "(?i)Road closed")
# 
# 
# write.csv(t_predict, "tree_model_road_closed1.csv")
# 
# 
# length(t_predict)
# 
# # incident is an indicator for mild
# # use closed as an indicator for severe instead of road closed
```

Random Forest experimentation

```{r}

# rf_train <- randomForest(as.factor(Severity) ~ ., data = train_mod, mtry = 3, importance = T)
# 
# head(train_mod)
# train_mod$Astronomical_Twilight <- as.factor(train_mod$Astronomical_Twilight)
# train_mod$weather <- as.factor(train_mod$weather)
# train_mod$rdclosed <- as.factor(train_mod$rdclosed)
# train_mod$altrt <- as.factor(train_mod$altrt)
# train_mod$caution <- as.factor(train_mod$caution)
# train_mod$road <- as.factor(train_mod$road)
# train_mod$closed <- as.factor(train_mod$closed)
# train_mod$blocked <- as.factor(train_mod$blocked)
# train_mod$incident <- as.factor(train_mod$incident)
# 
# head(test_mod)
# test_mod$Astronomical_Twilight <- as.factor(test_mod$Astronomical_Twilight)
# test_mod$weather <- as.factor(test_mod$weather)
# test_mod$rdclosed <- as.factor(test_mod$rdclosed)
# test_mod$altrt <- as.factor(test_mod$altrt)
# test_mod$caution <- as.factor(test_mod$caution)
# test_mod$road <- as.factor(test_mod$road)
# test_mod$closed <- as.factor(test_mod$closed)
# test_mod$blocked <- as.factor(test_mod$blocked)
# test_mod$incident <- as.factor(test_mod$incident)
# 
# test_mod$Astronomical_Twilight <- ifelse(is.na(test_mod$Astronomical_Twilight), "Day", test_mod$Astronomical_Twilight)
# 
# sum(is.na(test_mod))
# 
# d <- c()
# for(i in 1:35000) {
#   if(sum(is.na(train_mod[i,] > 0))) {
#     d <- c(d, i)
#   }
# }
# d
# train_mod[d[1:20],]
# 
# for(i in 1:35000) {
#   if(is.na(train_mod[i,]$Astronomical_Twilight)) {
#     train_mod[i,]$Astronomical_Twilight <- "Day"
#   }
# }
# summary(rf_train)
# rf_train_prediction <- predict(rf_train, data = train_mod, newdata = test_mod)
# length(rf_train_prediction)
# 
# 
# rf_train_sam <- predict(rf_train, data = train_sam, newdata = test_sam)
# table(rf_train_sam, test_sam$Severity)
# mean(rf_train_sam != test_sam$Severity)
# 
# write.csv(rf_train_prediction, "rand.forest1.csv")




```

XGBoost - Did not run for this model

```{r}
# library(gbm)
# head(train_mod)
# train_mod$time_diff <- as.numeric(train_mod$time_diff)
# train_mod$Severity <- ifelse(train_mod$Severity == "SEVERE", 0, 1)
# table(train_mod$Severity)
# boost_model <- gbm(as.factor(Severity) ~ ., data = train_mod, distribution = "bernoulli", n.trees = 5000, interaction.depth = 4)
# summary(boost_model)
# boost_train <- predict(boost_model, n.trees = 5000, newdata = test_mod)
# write.csv(boost_train, "rand.forest2.csv")
```

Experiment utilizing time

```{r}
# experiment

# train_severe <- train[train$Severity == "SEVERE",]
# train_mild <- train[train$Severity == "MILD",]
# head(train_severe[,c(1, 10)], 20)
# head(train_mild[,c(1, 10)], 20)
# head(train_severe, 30)
# # indexes are 1, 52, 51, 50, 49, 48, 47, 46, 45, 44
# 
# 
# start <- strsplit(train_severe$Start_Time[1], "T|Z")[[1]]
# end <- strsplit(train_severe$End_Time[1], "T|Z")[[1]]
# start <- paste(start[1], start[2], sep = " ")
# start
# end <- paste(end[1], end[2], sep = " ")
# end
# 
# end <- train_severe$End_Time[1]
# start <- train_severe$Start_Time[1]
#
# time <- strptime(start, format = "%Y-%m-%d %H:%M:%S")
#
# difftime(end, start, unit = "hours")
#
# difftime(train_severe$End_Time[1], train_severe$Start_Time[1], unit = "min")
# start <- strsplit(train_severe$Start_Time, "T|Z")
# end <- strsplit(train_severe$End_Time, "T|Z")
#
# end[[1]][1]
#
# difftime(end, start, unit = "hours")
# for(i in 1:length(end)) {
#   end[i] <- paste(end[[i]][1], end[[i]][2], sep = " ")
# }
# for(i in 1:length(start)) {
#   start[i] <- paste(start[[i]][1], start[[i]][2], sep = " ")
# }
#
# unlist(end)
#
# difftime(unlist(end), unlist(start), unit = "hours")
# end[[6]]
# start[[6]]
#
# train$time_diff <- difftime(unlist(end), unlist(start), unit = "hours")
#
# length(difftime(unlist(end), unlist(start), unit = "hours"))
#
# head(train_severe, 10)
```
